{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:44:34.999588Z",
     "start_time": "2025-11-27T07:44:31.542309Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, round\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894be9d61d17e358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:44:35.077278Z",
     "start_time": "2025-11-27T07:44:35.075719Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    TRAIN_PATH = '../data/train.parquet'\n",
    "    TEST_PATH = '../data/test.parquet'\n",
    "    LISTINGS_PATH = '../data/listings.parquet'\n",
    "    MODEL_PATH = '../data/xgboost_model_baseline'\n",
    "\n",
    "    # XGBoost Parameters\n",
    "    N_ESTIMATORS = 100\n",
    "    MAX_DEPTH = 6\n",
    "    LEARNING_RATE = 0.1\n",
    "    SUBSAMPLE = 0.8\n",
    "    COL_SAMPLE_BY_TREE = 0.8\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a195b02a8d88f1ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:44:38.245136Z",
     "start_time": "2025-11-27T07:44:35.084647Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/27 14:44:36 WARN Utils: Your hostname, nnnnnn.local, resolves to a loopback address: 127.0.0.1; using 192.168.70.243 instead (on interface en0)\n",
      "25/11/27 14:44:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/27 14:44:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/27 14:44:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/27 14:44:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/11/27 14:44:37 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created. Version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AirbnbXGBoost_Baseline\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Session created. Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781786c7957e2e01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:44:41.558470Z",
     "start_time": "2025-11-27T07:44:38.253846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count: 50,410\n",
      "Test count:  12,603\n",
      "Listings count: 12,004\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "\n",
    "if not os.path.exists(config.TRAIN_PATH) or not os.path.exists(config.TEST_PATH):\n",
    "    raise FileNotFoundError(\"Train/Test data not found. Please run the previous data prep step first.\")\n",
    "\n",
    "if not os.path.exists(config.LISTINGS_PATH):\n",
    "    raise FileNotFoundError(\"Listings data not found. Please run the previous data prep step first.\")\n",
    "\n",
    "# Load Spark DataFrames\n",
    "train_spark = spark.read.parquet(config.TRAIN_PATH)\n",
    "test_spark = spark.read.parquet(config.TEST_PATH)\n",
    "listings_spark = spark.read.parquet(config.LISTINGS_PATH)\n",
    "\n",
    "# Cache for faster iteration\n",
    "train_spark.cache()\n",
    "test_spark.cache()\n",
    "listings_spark.cache()\n",
    "\n",
    "print(f\"Train count: {train_spark.count():,}\")\n",
    "print(f\"Test count:  {test_spark.count():,}\")\n",
    "print(f\"Listings count: {listings_spark.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee23604d836831f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:44:41.655538Z",
     "start_time": "2025-11-27T07:44:41.571890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating features for XGBoost...\n",
      "  - Computing user statistics...\n",
      "  - Computing item statistics...\n",
      "  - Joining features to train data...\n",
      "  - Joining listings metadata...\n",
      "  - Applying features to test data...\n",
      "✓ Feature engineering complete\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "print(\"\\nCreating features for XGBoost...\")\n",
    "\n",
    "# 1. User features from training data\n",
    "print(\"  - Computing user statistics...\")\n",
    "user_stats = train_spark.groupBy(\"user_id\").agg(\n",
    "    avg(\"rating\").alias(\"user_avg_rating\"),\n",
    "    count(\"item_id\").alias(\"user_review_count\")\n",
    ").withColumnRenamed(\"user_id\", \"user_id_stats\")\n",
    "\n",
    "# 2. Item features from training data\n",
    "print(\"  - Computing item statistics...\")\n",
    "item_stats = train_spark.groupBy(\"item_id\").agg(\n",
    "    avg(\"rating\").alias(\"item_avg_rating\"),\n",
    "    count(\"user_id\").alias(\"item_review_count\")\n",
    ").withColumnRenamed(\"item_id\", \"item_id_stats\")\n",
    "\n",
    "# 3. Join user and item stats to train data\n",
    "print(\"  - Joining features to train data...\")\n",
    "train_with_features = train_spark \\\n",
    "    .join(user_stats, train_spark.user_id == user_stats.user_id_stats, \"left\") \\\n",
    "    .join(item_stats, train_spark.item_id == item_stats.item_id_stats, \"left\") \\\n",
    "    .drop(\"user_id_stats\", \"item_id_stats\")\n",
    "\n",
    "# 4. Join listings features\n",
    "print(\"  - Joining listings metadata...\")\n",
    "# Map listing_id to item_id if needed, or join directly if listing_id exists\n",
    "train_with_features = train_with_features \\\n",
    "    .join(listings_spark, train_with_features.listing_id == listings_spark.listing_id, \"left\")\n",
    "\n",
    "# 5. Apply same feature engineering to test data\n",
    "print(\"  - Applying features to test data...\")\n",
    "test_with_features = test_spark \\\n",
    "    .join(user_stats, test_spark.user_id == user_stats.user_id_stats, \"left\") \\\n",
    "    .join(item_stats, test_spark.item_id == item_stats.item_id_stats, \"left\") \\\n",
    "    .drop(\"user_id_stats\", \"item_id_stats\") \\\n",
    "    .join(listings_spark, test_spark.listing_id == listings_spark.listing_id, \"left\")\n",
    "\n",
    "print(\"✓ Feature engineering complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59f53ed3907e2970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:44:43.601870Z",
     "start_time": "2025-11-27T07:44:41.672436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data for XGBoost...\n",
      "  - Converting to Pandas DataFrames...\n",
      "  - Train shape: (50410, 29)\n",
      "  - Test shape: (12603, 29)\n",
      "  - Using 22 features\n",
      "✓ Data prepared for XGBoost\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for XGBoost\n",
    "print(\"\\nPreparing data for XGBoost...\")\n",
    "\n",
    "# Convert to Pandas for XGBoost\n",
    "print(\"  - Converting to Pandas DataFrames...\")\n",
    "train_df = train_with_features.toPandas()\n",
    "test_df = test_with_features.toPandas()\n",
    "\n",
    "print(f\"  - Train shape: {train_df.shape}\")\n",
    "print(f\"  - Test shape: {test_df.shape}\")\n",
    "\n",
    "# Select feature columns\n",
    "feature_cols = [\n",
    "    'user_id', 'item_id',\n",
    "    'user_avg_rating', 'user_review_count',\n",
    "    'item_avg_rating', 'item_review_count',\n",
    "    'price', 'accommodates', 'bedrooms', 'beds',\n",
    "    'minimum_nights', 'number_of_reviews',\n",
    "    'review_scores_rating', 'review_scores_location', 'review_scores_value',\n",
    "    'latitude', 'longitude'\n",
    "]\n",
    "\n",
    "# Handle categorical columns\n",
    "categorical_cols = ['property_type', 'room_type', 'neighbourhood_cleansed']\n",
    "label_encoders = {}\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    if col_name in train_df.columns:\n",
    "        le = LabelEncoder()\n",
    "        # Combine train and test for encoding\n",
    "        combined = pd.concat([train_df[col_name].fillna('unknown'), \n",
    "                              test_df[col_name].fillna('unknown')])\n",
    "        le.fit(combined)\n",
    "        train_df[f'{col_name}_encoded'] = le.transform(train_df[col_name].fillna('unknown'))\n",
    "        test_df[f'{col_name}_encoded'] = le.transform(test_df[col_name].fillna('unknown'))\n",
    "        label_encoders[col_name] = le\n",
    "        feature_cols.append(f'{col_name}_encoded')\n",
    "\n",
    "# Handle boolean columns\n",
    "bool_cols = ['host_is_superhost', 'instant_bookable']\n",
    "for col_name in bool_cols:\n",
    "    if col_name in train_df.columns:\n",
    "        train_df[col_name] = train_df[col_name].astype(float).fillna(0)\n",
    "        test_df[col_name] = test_df[col_name].astype(float).fillna(0)\n",
    "        feature_cols.append(col_name)\n",
    "\n",
    "# Select only available features\n",
    "available_features = [f for f in feature_cols if f in train_df.columns]\n",
    "print(f\"  - Using {len(available_features)} features\")\n",
    "\n",
    "# Prepare X and y\n",
    "X_train = train_df[available_features].fillna(0)\n",
    "y_train = train_df['rating'].values\n",
    "X_test = test_df[available_features].fillna(0)\n",
    "y_test = test_df['rating'].values\n",
    "\n",
    "print(\"✓ Data prepared for XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60def28480728a43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:44:43.885992Z",
     "start_time": "2025-11-27T07:44:43.629580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost model...\n",
      "✓ Model trained successfully\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost Model\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=config.N_ESTIMATORS,\n",
    "    max_depth=config.MAX_DEPTH,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    subsample=config.SUBSAMPLE,\n",
    "    colsample_bytree=config.COL_SAMPLE_BY_TREE,\n",
    "    random_state=config.RANDOM_STATE,\n",
    "    objective='reg:squarederror',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"✓ Model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "948033f970bd6b5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:44:43.921267Z",
     "start_time": "2025-11-27T07:44:43.908634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating predictions on test set...\n",
      "Sample Predictions:\n",
      " user_id  item_id  rating  prediction\n",
      "   13497       22    4.49    4.331111\n",
      "    7614     6588    4.62    4.790527\n",
      "      29     2317    3.68    4.104735\n",
      "   13131     1166    1.36    3.069667\n",
      "   14631        9    4.40    2.786867\n",
      "    4731     5110    3.90    4.362851\n",
      "     825     5346    3.29    3.406664\n",
      "   14301      618    4.39    2.658826\n",
      "    2732     4448    3.05    3.136234\n",
      "    3539      840    3.65    4.348840\n"
     ]
    }
   ],
   "source": [
    "# Generate Predictions\n",
    "print(\"\\nGenerating predictions on test set...\")\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Clip predictions to valid rating range [1, 5]\n",
    "y_pred = np.clip(y_pred, 1.0, 5.0)\n",
    "\n",
    "# Create predictions DataFrame for display\n",
    "predictions_df = pd.DataFrame({\n",
    "    'user_id': test_df['user_id'].values,\n",
    "    'item_id': test_df['item_id'].values,\n",
    "    'rating': y_test,\n",
    "    'prediction': y_pred\n",
    "})\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(predictions_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bcb7b6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:44:43.945684Z",
     "start_time": "2025-11-27T07:44:43.938877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating RMSE...\n",
      "------------------------------------------------\n",
      "Root Mean Square Error (RMSE): 1.0269\n",
      "------------------------------------------------\n",
      "\n",
      "Interpretation:\n",
      "On average, the model's prediction is off by 1.03 stars.\n",
      "For a 5-star scale, an RMSE below 1.0 is generally considered acceptable for a baseline.\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "          feature  importance\n",
      "  user_avg_rating    0.578336\n",
      "  item_avg_rating    0.134606\n",
      "   minimum_nights    0.088266\n",
      "host_is_superhost    0.027515\n",
      "room_type_encoded    0.023975\n",
      " instant_bookable    0.023321\n",
      "             beds    0.015884\n",
      "item_review_count    0.015334\n",
      "            price    0.011463\n",
      "          item_id    0.010985\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE\n",
    "print(\"\\nCalculating RMSE...\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Contextual Interpretation\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"On average, the model's prediction is off by {rmse:.2f} stars.\")\n",
    "print(f\"For a 5-star scale, an RMSE below 1.0 is generally considered acceptable for a baseline.\")\n",
    "\n",
    "# Feature Importance\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': available_features,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cefbdf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:44:44.038554Z",
     "start_time": "2025-11-27T07:44:44.025433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to ../data/xgboost_model_baseline...\n",
      "✓ Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Save the model for future use\n",
    "print(f\"\\nSaving model to {config.MODEL_PATH}...\")\n",
    "os.makedirs(config.MODEL_PATH, exist_ok=True)\n",
    "model.save_model(f\"{config.MODEL_PATH}/xgboost_model.json\")\n",
    "\n",
    "# Also save feature names and label encoders info\n",
    "import json\n",
    "model_info = {\n",
    "    'feature_names': available_features,\n",
    "    'categorical_columns': list(label_encoders.keys()),\n",
    "    'rmse': float(rmse)\n",
    "}\n",
    "\n",
    "with open(f\"{config.MODEL_PATH}/model_info.json\", 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(\"✓ Model saved successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
