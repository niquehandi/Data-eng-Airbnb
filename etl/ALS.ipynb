{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-27T07:57:07.355005Z",
     "start_time": "2025-11-27T07:57:05.581969Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, round, stddev, min as spark_min, max as spark_max\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "894be9d61d17e358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:57:07.363485Z",
     "start_time": "2025-11-27T07:57:07.361313Z"
    }
   },
   "source": [
    "class Config:\n",
    "    TRAIN_PATH = '../data/train.parquet'\n",
    "    TEST_PATH = '../data/test.parquet'\n",
    "    LISTINGS_PATH = '../data/listings.parquet'\n",
    "    MODEL_PATH = '../data/xgboost_model_baseline'\n",
    "\n",
    "    # Optimized XGBoost Parameters\n",
    "    N_ESTIMATORS = 200  # Increased for better learning\n",
    "    MAX_DEPTH = 5  # Reduced to prevent overfitting\n",
    "    LEARNING_RATE = 0.05  # Reduced for better convergence\n",
    "    SUBSAMPLE = 0.85\n",
    "    COL_SAMPLE_BY_TREE = 0.85\n",
    "    MIN_CHILD_WEIGHT = 3  # Regularization\n",
    "    GAMMA = 0.1  # Minimum loss reduction\n",
    "    REG_ALPHA = 0.1  # L1 regularization\n",
    "    REG_LAMBDA = 1.0  # L2 regularization\n",
    "    VALIDATION_SIZE = 0.2  # Validation set size for monitoring\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "config = Config()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a195b02a8d88f1ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:57:10.434678Z",
     "start_time": "2025-11-27T07:57:07.454283Z"
    }
   },
   "source": [
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AirbnbXGBoost_Baseline\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Session created. Version: {spark.version}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/27 14:57:08 WARN Utils: Your hostname, nnnnnn.local, resolves to a loopback address: 127.0.0.1; using 192.168.70.243 instead (on interface en0)\n",
      "25/11/27 14:57:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/27 14:57:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/27 14:57:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/27 14:57:09 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/11/27 14:57:09 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created. Version: 4.0.1\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "781786c7957e2e01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:57:13.911372Z",
     "start_time": "2025-11-27T07:57:10.469148Z"
    }
   },
   "source": [
    "print(\"Loading data...\")\n",
    "\n",
    "if not os.path.exists(config.TRAIN_PATH) or not os.path.exists(config.TEST_PATH):\n",
    "    raise FileNotFoundError(\"Train/Test data not found. Please run the previous data prep step first.\")\n",
    "\n",
    "if not os.path.exists(config.LISTINGS_PATH):\n",
    "    raise FileNotFoundError(\"Listings data not found. Please run the previous data prep step first.\")\n",
    "\n",
    "# Load Spark DataFrames\n",
    "train_spark = spark.read.parquet(config.TRAIN_PATH)\n",
    "test_spark = spark.read.parquet(config.TEST_PATH)\n",
    "listings_spark = spark.read.parquet(config.LISTINGS_PATH)\n",
    "\n",
    "# Cache for faster iteration\n",
    "train_spark.cache()\n",
    "test_spark.cache()\n",
    "listings_spark.cache()\n",
    "\n",
    "print(f\"Train count: {train_spark.count():,}\")\n",
    "print(f\"Test count:  {test_spark.count():,}\")\n",
    "print(f\"Listings count: {listings_spark.count():,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count: 50,410\n",
      "Test count:  12,603\n",
      "Listings count: 12,004\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "eee23604d836831f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:57:14.065773Z",
     "start_time": "2025-11-27T07:57:13.924449Z"
    }
   },
   "source": [
    "# Enhanced Feature Engineering\n",
    "print(\"\\nCreating enhanced features for XGBoost...\")\n",
    "\n",
    "# 1. Enhanced User features from training data\n",
    "print(\"  - Computing enhanced user statistics...\")\n",
    "user_stats = train_spark.groupBy(\"user_id\").agg(\n",
    "    avg(\"rating\").alias(\"user_avg_rating\"),\n",
    "    stddev(\"rating\").alias(\"user_rating_std\"),\n",
    "    count(\"item_id\").alias(\"user_review_count\"),\n",
    "    spark_min(\"rating\").alias(\"user_min_rating\"),\n",
    "    spark_max(\"rating\").alias(\"user_max_rating\")\n",
    ").withColumnRenamed(\"user_id\", \"user_id_stats\")\n",
    "\n",
    "# 2. Enhanced Item features from training data\n",
    "print(\"  - Computing enhanced item statistics...\")\n",
    "item_stats = train_spark.groupBy(\"item_id\").agg(\n",
    "    avg(\"rating\").alias(\"item_avg_rating\"),\n",
    "    stddev(\"rating\").alias(\"item_rating_std\"),\n",
    "    count(\"user_id\").alias(\"item_review_count\"),\n",
    "    spark_min(\"rating\").alias(\"item_min_rating\"),\n",
    "    spark_max(\"rating\").alias(\"item_max_rating\")\n",
    ").withColumnRenamed(\"item_id\", \"item_id_stats\")\n",
    "\n",
    "# 3. Join enhanced user and item stats to train data\n",
    "print(\"  - Joining enhanced features to train data...\")\n",
    "train_with_features = train_spark \\\n",
    "    .join(user_stats, train_spark.user_id == user_stats.user_id_stats, \"left\") \\\n",
    "    .join(item_stats, train_spark.item_id == item_stats.item_id_stats, \"left\") \\\n",
    "    .drop(\"user_id_stats\", \"item_id_stats\")\n",
    "\n",
    "# 4. Join listings features\n",
    "print(\"  - Joining listings metadata...\")\n",
    "train_with_features = train_with_features \\\n",
    "    .join(listings_spark, train_with_features.listing_id == listings_spark.listing_id, \"left\")\n",
    "\n",
    "# 5. Apply same feature engineering to test data\n",
    "print(\"  - Applying enhanced features to test data...\")\n",
    "test_with_features = test_spark \\\n",
    "    .join(user_stats, test_spark.user_id == user_stats.user_id_stats, \"left\") \\\n",
    "    .join(item_stats, test_spark.item_id == item_stats.item_id_stats, \"left\") \\\n",
    "    .drop(\"user_id_stats\", \"item_id_stats\") \\\n",
    "    .join(listings_spark, test_spark.listing_id == listings_spark.listing_id, \"left\")\n",
    "\n",
    "print(\"✓ Enhanced feature engineering complete\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating enhanced features for XGBoost...\n",
      "  - Computing enhanced user statistics...\n",
      "  - Computing enhanced item statistics...\n",
      "  - Joining enhanced features to train data...\n",
      "  - Joining listings metadata...\n",
      "  - Applying enhanced features to test data...\n",
      "✓ Enhanced feature engineering complete\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "59f53ed3907e2970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:57:16.288849Z",
     "start_time": "2025-11-27T07:57:14.083317Z"
    }
   },
   "source": [
    "# Prepare data for XGBoost\n",
    "print(\"\\nPreparing data for XGBoost...\")\n",
    "\n",
    "# Convert to Pandas for XGBoost\n",
    "print(\"  - Converting to Pandas DataFrames...\")\n",
    "train_df = train_with_features.toPandas()\n",
    "test_df = test_with_features.toPandas()\n",
    "\n",
    "print(f\"  - Train shape: {train_df.shape}\")\n",
    "print(f\"  - Test shape: {test_df.shape}\")\n",
    "\n",
    "# Base feature columns\n",
    "feature_cols = [\n",
    "    'user_id', 'item_id',\n",
    "    'user_avg_rating', 'user_rating_std', 'user_review_count',\n",
    "    'user_min_rating', 'user_max_rating',\n",
    "    'item_avg_rating', 'item_rating_std', 'item_review_count',\n",
    "    'item_min_rating', 'item_max_rating',\n",
    "    'price', 'accommodates', 'bedrooms', 'beds',\n",
    "    'minimum_nights', 'number_of_reviews',\n",
    "    'review_scores_rating', 'review_scores_location', 'review_scores_value',\n",
    "    'latitude', 'longitude'\n",
    "]\n",
    "\n",
    "# Handle categorical columns\n",
    "categorical_cols = ['property_type', 'room_type', 'neighbourhood_cleansed']\n",
    "label_encoders = {}\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    if col_name in train_df.columns:\n",
    "        le = LabelEncoder()\n",
    "        # Combine train and test for encoding\n",
    "        combined = pd.concat([train_df[col_name].fillna('unknown'), \n",
    "                              test_df[col_name].fillna('unknown')])\n",
    "        le.fit(combined)\n",
    "        train_df[f'{col_name}_encoded'] = le.transform(train_df[col_name].fillna('unknown'))\n",
    "        test_df[f'{col_name}_encoded'] = le.transform(test_df[col_name].fillna('unknown'))\n",
    "        label_encoders[col_name] = le\n",
    "        feature_cols.append(f'{col_name}_encoded')\n",
    "\n",
    "# Handle boolean columns\n",
    "bool_cols = ['host_is_superhost', 'instant_bookable']\n",
    "for col_name in bool_cols:\n",
    "    if col_name in train_df.columns:\n",
    "        train_df[col_name] = train_df[col_name].astype(float).fillna(0)\n",
    "        test_df[col_name] = test_df[col_name].astype(float).fillna(0)\n",
    "        feature_cols.append(col_name)\n",
    "\n",
    "# Create derived features\n",
    "print(\"  - Creating derived features...\")\n",
    "\n",
    "# Price per person\n",
    "train_df['price_per_person'] = train_df['price'] / (train_df['accommodates'] + 1e-6)\n",
    "test_df['price_per_person'] = test_df['price'] / (test_df['accommodates'] + 1e-6)\n",
    "feature_cols.append('price_per_person')\n",
    "\n",
    "# Bedroom ratio\n",
    "train_df['bedroom_ratio'] = train_df['bedrooms'] / (train_df['accommodates'] + 1e-6)\n",
    "test_df['bedroom_ratio'] = test_df['bedrooms'] / (test_df['accommodates'] + 1e-6)\n",
    "feature_cols.append('bedroom_ratio')\n",
    "\n",
    "# Bed ratio\n",
    "train_df['bed_ratio'] = train_df['beds'] / (train_df['accommodates'] + 1e-6)\n",
    "test_df['bed_ratio'] = test_df['beds'] / (test_df['accommodates'] + 1e-6)\n",
    "feature_cols.append('bed_ratio')\n",
    "\n",
    "# Review score composite\n",
    "train_df['review_score_composite'] = (\n",
    "    train_df['review_scores_rating'].fillna(0) * 0.5 +\n",
    "    train_df['review_scores_location'].fillna(0) * 0.3 +\n",
    "    train_df['review_scores_value'].fillna(0) * 0.2\n",
    ")\n",
    "test_df['review_score_composite'] = (\n",
    "    test_df['review_scores_rating'].fillna(0) * 0.5 +\n",
    "    test_df['review_scores_location'].fillna(0) * 0.3 +\n",
    "    test_df['review_scores_value'].fillna(0) * 0.2\n",
    ")\n",
    "feature_cols.append('review_score_composite')\n",
    "\n",
    "# Interaction features\n",
    "print(\"  - Creating interaction features...\")\n",
    "train_df['user_avg_x_item_avg'] = train_df['user_avg_rating'] * train_df['item_avg_rating']\n",
    "test_df['user_avg_x_item_avg'] = test_df['user_avg_rating'] * test_df['item_avg_rating']\n",
    "feature_cols.append('user_avg_x_item_avg')\n",
    "\n",
    "train_df['user_avg_x_price_norm'] = train_df['user_avg_rating'] * (train_df['price'] / 100)\n",
    "test_df['user_avg_x_price_norm'] = test_df['user_avg_rating'] * (test_df['price'] / 100)\n",
    "feature_cols.append('user_avg_x_price_norm')\n",
    "\n",
    "train_df['item_avg_x_review_score'] = train_df['item_avg_rating'] * train_df['review_score_composite']\n",
    "test_df['item_avg_x_review_score'] = test_df['item_avg_rating'] * test_df['review_score_composite']\n",
    "feature_cols.append('item_avg_x_review_score')\n",
    "\n",
    "# Select only available features\n",
    "available_features = [f for f in feature_cols if f in train_df.columns]\n",
    "print(f\"  - Using {len(available_features)} features\")\n",
    "\n",
    "# Better missing value handling using median\n",
    "print(\"  - Handling missing values with median imputation...\")\n",
    "numeric_features = [f for f in available_features if f not in ['user_id', 'item_id']]\n",
    "for col in numeric_features:\n",
    "    if col in train_df.columns:\n",
    "        median_val = train_df[col].median()\n",
    "        if pd.notna(median_val):\n",
    "            train_df[col] = train_df[col].fillna(median_val)\n",
    "            test_df[col] = test_df[col].fillna(median_val)\n",
    "        else:\n",
    "            train_df[col] = train_df[col].fillna(0)\n",
    "            test_df[col] = test_df[col].fillna(0)\n",
    "\n",
    "# Prepare X and y\n",
    "X_train = train_df[available_features]\n",
    "y_train = train_df['rating'].values\n",
    "X_test = test_df[available_features]\n",
    "y_test = test_df['rating'].values\n",
    "\n",
    "print(\"✓ Data prepared for XGBoost\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data for XGBoost...\n",
      "  - Converting to Pandas DataFrames...\n",
      "  - Train shape: (50410, 35)\n",
      "  - Test shape: (12603, 35)\n",
      "  - Creating derived features...\n",
      "  - Creating interaction features...\n",
      "  - Using 35 features\n",
      "  - Handling missing values with median imputation...\n",
      "✓ Data prepared for XGBoost\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "60def28480728a43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:57:16.943539Z",
     "start_time": "2025-11-27T07:57:16.313783Z"
    }
   },
   "source": [
    "# Train XGBoost Model\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "\n",
    "# Create validation set for monitoring\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train, \n",
    "    test_size=config.VALIDATION_SIZE, \n",
    "    random_state=config.RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"  - Training set: {X_train_split.shape[0]:,} samples\")\n",
    "print(f\"  - Validation set: {X_val_split.shape[0]:,} samples\")\n",
    "\n",
    "# XGBoost 3.x sklearn API\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=config.N_ESTIMATORS,\n",
    "    max_depth=config.MAX_DEPTH,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    subsample=config.SUBSAMPLE,\n",
    "    colsample_bytree=config.COL_SAMPLE_BY_TREE,\n",
    "    min_child_weight=config.MIN_CHILD_WEIGHT,\n",
    "    gamma=config.GAMMA,\n",
    "    reg_alpha=config.REG_ALPHA,\n",
    "    reg_lambda=config.REG_LAMBDA,\n",
    "    random_state=config.RANDOM_STATE,\n",
    "    objective='reg:squarederror',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_pred = model.predict(X_val_split)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val_split, val_pred))\n",
    "print(f\"  - Validation RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "print(f\"✓ Model trained successfully ({config.N_ESTIMATORS} iterations)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost model...\n",
      "  - Training set: 40,328 samples\n",
      "  - Validation set: 10,082 samples\n",
      "  - Validation RMSE: 0.4144\n",
      "✓ Model trained successfully (200 iterations)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "948033f970bd6b5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:57:16.982084Z",
     "start_time": "2025-11-27T07:57:16.970455Z"
    }
   },
   "source": [
    "# Generate Predictions\n",
    "print(\"\\nGenerating predictions on test set...\")\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Clip predictions to valid rating range [1, 5]\n",
    "y_pred = np.clip(y_pred, 1.0, 5.0)\n",
    "\n",
    "# Create predictions DataFrame for display\n",
    "predictions_df = pd.DataFrame({\n",
    "    'user_id': test_df['user_id'].values,\n",
    "    'item_id': test_df['item_id'].values,\n",
    "    'rating': y_test,\n",
    "    'prediction': y_pred\n",
    "})\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(predictions_df.head(10).to_string(index=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating predictions on test set...\n",
      "Sample Predictions:\n",
      " user_id  item_id  rating  prediction\n",
      "   13497       22    4.49    4.141732\n",
      "    7614     6588    4.62    4.353892\n",
      "      29     2317    3.68    4.216444\n",
      "   13131     1166    1.36    3.310230\n",
      "   14631        9    4.40    2.964554\n",
      "    4731     5110    3.90    3.928891\n",
      "     825     5346    3.29    3.747145\n",
      "   14301      618    4.39    2.588744\n",
      "    2732     4448    3.05    3.779743\n",
      "    3539      840    3.65    3.394812\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "9bcb7b6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:57:17.020278Z",
     "start_time": "2025-11-27T07:57:17.011731Z"
    }
   },
   "source": [
    "# Calculate RMSE\n",
    "print(\"\\nCalculating RMSE...\")\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Contextual Interpretation\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"On average, the model's prediction is off by {rmse:.2f} stars.\")\n",
    "if rmse < 1.0:\n",
    "    print(f\"✓ Excellent! RMSE is below 1.0, which is considered good for a 5-star scale.\")\n",
    "else:\n",
    "    print(f\"For a 5-star scale, an RMSE below 1.0 is generally considered acceptable for a baseline.\")\n",
    "\n",
    "# Feature Importance\n",
    "print(\"\\nTop 15 Most Important Features:\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': available_features,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "# Calculate additional metrics\n",
    "print(\"\\nAdditional Metrics:\")\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Rating: {np.mean(y_test):.4f}\")\n",
    "print(f\"Std Rating: {np.std(y_test):.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating RMSE...\n",
      "------------------------------------------------\n",
      "Root Mean Square Error (RMSE): 0.9051\n",
      "------------------------------------------------\n",
      "\n",
      "Interpretation:\n",
      "On average, the model's prediction is off by 0.91 stars.\n",
      "✓ Excellent! RMSE is below 1.0, which is considered good for a 5-star scale.\n",
      "\n",
      "Top 15 Most Important Features:\n",
      "                feature  importance\n",
      "    user_avg_x_item_avg    0.722688\n",
      "        user_avg_rating    0.096293\n",
      "        user_max_rating    0.025276\n",
      "        item_max_rating    0.024909\n",
      "        item_avg_rating    0.019960\n",
      "        user_min_rating    0.019134\n",
      "        item_min_rating    0.009989\n",
      "item_avg_x_review_score    0.009695\n",
      "        user_rating_std    0.007250\n",
      "        item_rating_std    0.005606\n",
      "                item_id    0.005529\n",
      "      item_review_count    0.005109\n",
      "      host_is_superhost    0.004259\n",
      "         minimum_nights    0.003212\n",
      "      room_type_encoded    0.003024\n",
      "\n",
      "Additional Metrics:\n",
      "Mean Absolute Error (MAE): 0.6854\n",
      "Mean Rating: 2.9995\n",
      "Std Rating: 1.1583\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "2cefbdf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T07:57:17.058709Z",
     "start_time": "2025-11-27T07:57:17.044203Z"
    }
   },
   "source": [
    "# Save the model for future use\n",
    "print(f\"\\nSaving model to {config.MODEL_PATH}...\")\n",
    "os.makedirs(config.MODEL_PATH, exist_ok=True)\n",
    "model.save_model(f\"{config.MODEL_PATH}/xgboost_model.json\")\n",
    "\n",
    "# Also save feature names and model info\n",
    "model_info = {\n",
    "    'feature_names': available_features,\n",
    "    'categorical_columns': list(label_encoders.keys()),\n",
    "    'rmse': float(rmse),\n",
    "    'mae': float(mae),\n",
    "    'n_estimators_used': config.N_ESTIMATORS,\n",
    "    'n_features': len(available_features),\n",
    "    'hyperparameters': {\n",
    "        'n_estimators': config.N_ESTIMATORS,\n",
    "        'max_depth': config.MAX_DEPTH,\n",
    "        'learning_rate': config.LEARNING_RATE,\n",
    "        'subsample': config.SUBSAMPLE,\n",
    "        'colsample_bytree': config.COL_SAMPLE_BY_TREE,\n",
    "        'min_child_weight': config.MIN_CHILD_WEIGHT,\n",
    "        'gamma': config.GAMMA,\n",
    "        'reg_alpha': config.REG_ALPHA,\n",
    "        'reg_lambda': config.REG_LAMBDA\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{config.MODEL_PATH}/model_info.json\", 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv(f\"{config.MODEL_PATH}/feature_importance.csv\", index=False)\n",
    "\n",
    "print(\"✓ Model saved successfully\")\n",
    "print(f\"  - Model: {config.MODEL_PATH}/xgboost_model.json\")\n",
    "print(f\"  - Info: {config.MODEL_PATH}/model_info.json\")\n",
    "print(f\"  - Feature importance: {config.MODEL_PATH}/feature_importance.csv\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to ../data/xgboost_model_baseline...\n",
      "✓ Model saved successfully\n",
      "  - Model: ../data/xgboost_model_baseline/xgboost_model.json\n",
      "  - Info: ../data/xgboost_model_baseline/model_info.json\n",
      "  - Feature importance: ../data/xgboost_model_baseline/feature_importance.csv\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
