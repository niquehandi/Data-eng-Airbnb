# Airbnb Rating Prediction System

A comprehensive data engineering and machine learning project that predicts user ratings for Airbnb listings using XGBoost. This project processes raw Airbnb data, engineers features, and trains a gradient boosting model to predict ratings on a 1-5 scale.

## üìã Table of Contents

- [Overview](#overview)
- [Project Structure](#project-structure)
- [Features](#features)
- [Technologies Used](#technologies-used)
- [Installation](#installation)
- [Usage](#usage)
- [Data Pipeline](#data-pipeline)
- [Model Details](#model-details)
- [Results](#results)
- [Documentation](#documentation)

## üéØ Overview

This project implements an end-to-end machine learning pipeline for predicting Airbnb listing ratings. The system:

- Processes raw Airbnb listings and reviews data
- Engineers comprehensive features from user behavior, listing properties, and interactions
- Trains an XGBoost regression model to predict ratings
- Achieves RMSE < 1.0 on a 5-star rating scale

The model helps understand what factors influence user satisfaction with Airbnb listings, enabling better recommendations and insights for both hosts and guests.

## üìÅ Project Structure

```
Data-eng-Airbnb/
‚îú‚îÄ‚îÄ data/                          # Data files (CSV, Parquet)
‚îÇ   ‚îú‚îÄ‚îÄ listings.csv               # Raw listings data
‚îÇ   ‚îú‚îÄ‚îÄ reviews.csv                # Raw reviews data
‚îÇ   ‚îú‚îÄ‚îÄ train.parquet/             # Training dataset
‚îÇ   ‚îú‚îÄ‚îÄ test.parquet/              # Test dataset
‚îÇ   ‚îú‚îÄ‚îÄ listings.parquet/          # Processed listings
‚îÇ   ‚îú‚îÄ‚îÄ user_id_mapping.csv        # User ID mappings
‚îÇ   ‚îú‚îÄ‚îÄ item_id_mapping.csv        # Listing ID mappings
‚îÇ   ‚îî‚îÄ‚îÄ xgboost_model_baseline/    # Trained model artifacts
‚îÇ       ‚îú‚îÄ‚îÄ xgboost_model.json     # Saved model
‚îÇ       ‚îú‚îÄ‚îÄ model_info.json         # Model metadata
‚îÇ       ‚îî‚îÄ‚îÄ feature_importance.csv  # Feature importance scores
‚îú‚îÄ‚îÄ etl/                           # ETL notebooks
‚îÇ   ‚îú‚îÄ‚îÄ Data Preprocessing.ipynb   # Data cleaning and feature engineering
‚îÇ   ‚îî‚îÄ‚îÄ ALS.ipynb                  # Model training (XGBoost)
‚îú‚îÄ‚îÄ docs/                          # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ rating_normalization.md    # Rating normalization methodology
‚îú‚îÄ‚îÄ venv/                          # Python virtual environment
‚îî‚îÄ‚îÄ README.md                      # This file
```

## ‚ú® Features

### Data Processing
- **Data Cleaning**: Handles missing values, data type conversions, and data quality checks
- **Filtering**: Filters users and listings based on minimum review thresholds
- **Rating Construction**: Creates implicit ratings from review engagement signals (recency, length, frequency)
- **Feature Engineering**: Creates 24 clean features including:
  - User statistics (average rating, review count, rating variance)
  - Item statistics (average rating, popularity metrics)
  - Listing properties (price, location, amenities)
  - Derived features (price per person, bedroom ratios)
  - Interaction features (user-item combinations)

### Machine Learning
- **XGBoost Regression**: Gradient boosting model optimized for rating prediction
- **Feature Selection**: 24 clean features from listing metadata (no target-derived leakage)
- **Hyperparameter Tuning**: Optimized parameters for regularization and generalization
- **Model Evaluation**: Comprehensive metrics (RMSE, MAE) with validation monitoring

## üõ† Technologies Used

- **Python 3.13**: Core programming language
- **Apache Spark 4.0.1**: Distributed data processing
- **XGBoost 3.1.2**: Gradient boosting machine learning
- **Pandas 2.3.3**: Data manipulation and analysis
- **NumPy**: Numerical computing
- **Scikit-learn**: Machine learning utilities
- **Jupyter Notebooks**: Interactive development environment
- **Matplotlib**: Data visualization

## üì¶ Installation

### Prerequisites

- Python 3.13 or higher
- Java 8 or higher (required for Apache Spark)
- 4GB+ RAM recommended

### Setup Steps

1. **Clone the repository** (if applicable):
   ```bash
   git clone <repository-url>
   cd Data-eng-Airbnb
   ```

2. **Create and activate virtual environment**:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**:
   ```bash
   pip install pyspark pandas numpy xgboost scikit-learn matplotlib jupyter
   ```

4. **Verify Spark installation**:
   ```bash
   python -c "from pyspark.sql import SparkSession; print('Spark installed successfully')"
   ```

## üöÄ Usage

### Step 1: Data Preprocessing

Run the data preprocessing notebook to clean and prepare the data:

```bash
jupyter notebook etl/Data\ Preprocessing.ipynb
```

This notebook:
- Loads raw listings and reviews data
- Applies filtering thresholds (minimum reviews per user/listing)
- Constructs implicit ratings from review signals
- Creates train/test splits (80/20)
- Generates user and item ID mappings
- Saves processed data as Parquet files

**Key Configuration** (in notebook):
```python
MIN_USER_REVIEWS = 5      # Minimum reviews per user
MIN_LISTING_REVIEWS = 10  # Minimum reviews per listing
TRAIN_RATIO = 0.8         # Train/test split ratio
```

### Step 2: Model Training

Run the model training notebook:

```bash
jupyter notebook etl/ALS.ipynb
```

This notebook:
- Loads preprocessed training and test data
- Engineers additional features (user/item stats, interactions)
- Trains XGBoost regression model
- Evaluates model performance
- Saves trained model and metadata

**Model Configuration**:
```python
N_ESTIMATORS = 200        # Number of boosting rounds
MAX_DEPTH = 5             # Maximum tree depth
LEARNING_RATE = 0.05      # Learning rate
REG_ALPHA = 0.1           # L1 regularization
REG_LAMBDA = 1.0          # L2 regularization
```

### Step 3: Model Evaluation

The training notebook automatically:
- Calculates RMSE and MAE on test set
- Displays feature importance rankings
- Saves model artifacts to `data/xgboost_model_baseline/`

## üîÑ Data Pipeline

### Input Data
- **Listings**: Property details (price, location, amenities, host info)
- **Reviews**: User reviews with timestamps and comments

### Processing Steps

1. **Data Loading**: Read CSV files into Spark DataFrames
2. **Filtering**: Keep only active users (‚â•5 reviews) and popular listings (‚â•10 reviews)
3. **Rating Construction**: Create implicit ratings from:
   - Review recency (exponential decay)
   - Review length (log-transformed)
   - User credibility (review count)
   - Listing popularity
4. **Feature Engineering**:
   - User-level aggregations (avg rating, count, variance)
   - Item-level aggregations (avg rating, popularity)
   - Listing metadata (price, location, amenities)
   - Derived features (ratios, composites)
   - Interaction features (user √ó item combinations)
5. **Train/Test Split**: 80/20 split maintaining user overlap
6. **Model Training**: XGBoost with optimized hyperparameters

### Output Data
- **Train/Test Parquet**: Processed datasets ready for modeling
- **ID Mappings**: User and listing ID mappings for reference
- **Trained Model**: XGBoost model in JSON format
- **Model Metadata**: Performance metrics and feature information

## ü§ñ Model Details

### Model Architecture
- **Algorithm**: XGBoost Gradient Boosting Regressor
- **Objective**: Regression (squared error)
- **Features**: 24 clean features (no target-derived leakage)
- **Training**: 200 boosting rounds with regularization

### Feature Categories

1. **User Features** (7 features):
   - `user_avg_rating`: Average rating given by user
   - `user_rating_std`: Standard deviation of user ratings
   - `user_review_count`: Number of reviews by user
   - `user_min_rating`, `user_max_rating`: Rating range
   - User ID (encoded)

2. **Item Features** (7 features):
   - `item_avg_rating`: Average rating for listing
   - `item_rating_std`: Standard deviation of listing ratings
   - `item_review_count`: Number of reviews for listing
   - `item_min_rating`, `item_max_rating`: Rating range
   - Item ID (encoded)

3. **Listing Features** (15 features):
   - Price, location (latitude/longitude)
   - Accommodation details (bedrooms, beds, accommodates)
   - Review scores (rating, location, value)
   - Host features (superhost status, instant bookable)
   - Property characteristics (type, room type, neighbourhood)

4. **Derived Features** (6 features):
   - `price_per_person`: Price divided by accommodates
   - `bedroom_ratio`, `bed_ratio`: Space per person
   - `review_score_composite`: Weighted review score
   - Interaction features (user √ó item combinations)

### Hyperparameters

| Parameter | Value | Purpose |
|-----------|-------|---------|
| `n_estimators` | 200 | Number of boosting rounds |
| `max_depth` | 5 | Tree depth (prevents overfitting) |
| `learning_rate` | 0.05 | Step size shrinkage |
| `subsample` | 0.85 | Row sampling ratio |
| `colsample_bytree` | 0.85 | Column sampling ratio |
| `min_child_weight` | 3 | Minimum samples per leaf |
| `gamma` | 0.1 | Minimum loss reduction |
| `reg_alpha` | 0.1 | L1 regularization |
| `reg_lambda` | 1.0 | L2 regularization |

## üìä Results

### ‚ö†Ô∏è Data Leakage Fix (December 2025)

The original model had a **data leakage issue** with the `user_avg_x_item_avg` feature:
- This feature multiplied `user_avg_rating √ó item_avg_rating`
- Both values were derived from the training data's ratings (the target variable)
- This created a circular dependency: the feature encoded the target
- Result: artificially low error metrics (RMSE: 0.905, MAE: 0.685)

**The feature has been removed.** The model now uses 34 features instead of 35.

### Model Performance (No Data Leakage)

All target-derived features removed. The model now uses **only clean features**:

| Metric | Original (with leakage) | No Leakage | Change |
|--------|------------------------|------------|--------|
| RMSE | 0.905 | **0.896** | -1.0% ‚úÖ |
| MAE | 0.685 | **0.728** | +6.3% |
| Features | 35 | 24 | -11 |

**Key Finding:** RMSE actually **improved** while using only clean features!

### Top Features by Importance (No Leakage)

1. `minimum_nights` (44.1%): Listing policy - strongest predictor!
2. `host_is_superhost` (9.8%): Host quality indicator
3. `bed_ratio` (6.1%): Comfort metric (beds per person)
4. `instant_bookable` (3.9%): Convenience factor
5. `number_of_reviews` (3.2%): Popularity/trustworthiness
6. `beds` (3.2%): Capacity
7. `price_per_person` (2.9%): Value metric

### Interpretation

- The model learns from **real listing characteristics**, not rating proxies
- **Minimum nights** policy is the strongest predictor of satisfaction
- **Superhost status** provides a meaningful quality signal
- **Comfort metrics** (bed ratio, beds) matter for guest experience
- **No data leakage** ensures reliable generalization to new users/listings
- The model is now **fully interpretable** - we know what drives ratings!

## üìö Documentation

### Additional Resources

- **Rating Normalization**: See `docs/rating_normalization.md` for details on how implicit ratings are constructed from review signals
- **Model Artifacts**: Check `data/xgboost_model_baseline/model_info.json` for complete model configuration and performance metrics

### Key Concepts

**Implicit Ratings**: Since Airbnb doesn't provide explicit numeric ratings, the system constructs ratings from review engagement signals:
- **Recency**: More recent reviews weighted higher
- **Length**: Longer reviews indicate higher engagement
- **Frequency**: Multiple reviews suggest stronger preference
- **Composite Score**: Weighted combination mapped to 1-5 scale

**Feature Engineering Strategy**:
- Aggregated statistics capture user and item patterns
- Derived features capture relationships (ratios, composites)
- Interaction features capture personalized preferences
- Missing values handled with median imputation

## üîß Configuration

### Data Preprocessing Settings

Edit `etl/Data Preprocessing.ipynb` Config class:
```python
MIN_USER_REVIEWS = 5      # Adjust filtering threshold
MIN_LISTING_REVIEWS = 10  # Adjust filtering threshold
TRAIN_RATIO = 0.8         # Adjust train/test split
```

### Model Training Settings

Edit `etl/ALS.ipynb` Config class:
```python
N_ESTIMATORS = 200        # Adjust model complexity
MAX_DEPTH = 5             # Adjust tree depth
LEARNING_RATE = 0.05      # Adjust learning rate
```

## üêõ Troubleshooting

### Common Issues

1. **Spark Memory Errors**: Increase `spark.driver.memory` in notebook
2. **Missing Data Files**: Ensure data files are in `data/` directory
3. **XGBoost Version**: Project uses XGBoost 3.x API (callbacks not in fit())

### Performance Tips

- Use Parquet format for faster I/O
- Cache Spark DataFrames for repeated operations
- Adjust hyperparameters based on validation performance
- Monitor feature importance to identify redundant features

## üìù License

This project is for educational and research purposes.

## üë• Contributing

This is a data engineering project. For improvements:
1. Test changes on sample data first
2. Document new features
3. Update this README with changes

## üìß Contact

For questions or issues, please refer to the project documentation or create an issue in the repository.

---

**Last Updated**: December 2025  
**Version**: 2.0 (All data leakage removed)  
**Status**: ‚úÖ Production Ready - No Data Leakage
